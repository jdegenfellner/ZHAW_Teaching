}
# Now try the function with the gamma distribution
estimate_coverage_probability_gamma("rgamma", list(shape = 2, rate = 1), sample_size = 10)
for(i in 1:length(sizes)){
df$cov_prob[i] <- estimate_coverage_probability_gamma("rgamma", list(shape = 2, rate = 1),
sample_size = sizes[i])
}
df %>% ggplot(aes(x = sizes, y = cov_prob)) +
geom_line() +
geom_smooth() +
geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") +
ggtitle("(Simulated) Coverage probability of the CI for the mean (from CLT)") +
theme(plot.title = element_text(hjust = 0.5)) +
ylab("Simulated coverage probability") + xlab("Sample size (n)")
# GPT4 used.
library(pacman)
p_load(tidyverse)
x <- seq(-4, 4, length.out = 100)
normal_pdf <- dnorm(x)
t_pdf_29 <- dt(x, df = 29)
t_pdf_30 <- dt(x, df = 30)
df <- data.frame(
x = c(x, x, x),
density = c(normal_pdf, t_pdf_29, t_pdf_30),
Distribution = factor(c(rep("Normal", length(x)), rep("t (df=29)", length(x)), rep("t (df=30)", length(x))))
)
ggplot(df, aes(x = x, y = density, color = Distribution)) +
geom_line() +
labs(title = "Comparison of Normal and t-Distributions", x = "Value", y = "Density") +
scale_color_manual(values = c("blue", "red", "green")) +
theme_minimal()
# GPT4 used.
library(pacman)
p_load(tidyverse)
x_values <- seq(-4, 4, by = 0.01)
comparison_df <- data.frame(
x = c(x_values, x_values, x_values),
Density = c(dnorm(x_values), dt(x_values, df = 29), dt(x_values, df = 30)),
Distribution = factor(rep(c("Normal", "t (df = 29)", "t (df = 30)"), each = length(x_values)))
)
ggplot(comparison_df, aes(x = x, y = Density, color = Distribution)) +
geom_line() +
scale_color_manual(values = c("black", "red", "blue")) +
labs(title = "Comparison of Normal and t Distributions around n = 30",
x = "Z/T value",
y = "Density",
color = "Distribution") +
theme_minimal()
comparison_df <- data.frame(
x = c(x_values, x_values, x_values),
Density = c(dnorm(x_values), dt(x_values, df = 20), dt(x_values, df = 30)),
Distribution = factor(rep(c("Normal", "t (df = 20)", "t (df = 30)"), each = length(x_values)))
)
ggplot(comparison_df, aes(x = x, y = Density, color = Distribution)) +
geom_line() +
scale_color_manual(values = c("black", "red", "blue")) +
labs(title = "Comparison of Normal and t Distributions around n = 30",
x = "Z/T value",
y = "Density",
color = "Distribution") +
theme_minimal()
comparison_df <- data.frame(
x = c(x_values, x_values, x_values),
Density = c(dnorm(x_values), dt(x_values, df = 10), dt(x_values, df = 30)),
Distribution = factor(rep(c("Normal", "t (df = 100)", "t (df = 30)"), each = length(x_values)))
)
ggplot(comparison_df, aes(x = x, y = Density, color = Distribution)) +
geom_line() +
scale_color_manual(values = c("black", "red", "blue")) +
labs(title = "Comparison of Normal and t Distributions around n = 30",
x = "Z/T value",
y = "Density",
color = "Distribution") +
theme_minimal()
comparison_df <- data.frame(
x = c(x_values, x_values, x_values),
Density = c(dnorm(x_values), dt(x_values, df = 10), dt(x_values, df = 30)),
Distribution = factor(rep(c("Normal", "t (df = 10)", "t (df = 30)"), each = length(x_values)))
)
ggplot(comparison_df, aes(x = x, y = Density, color = Distribution)) +
geom_line() +
scale_color_manual(values = c("black", "red", "blue")) +
labs(title = "Comparison of Normal and t Distributions around n = 30",
x = "Z/T value",
y = "Density",
color = "Distribution") +
theme_minimal()
?wilcox.test
?rgamma
# Set a seed for reproducibility
set.seed(123)
# Generate skewed data from a Gamma distribution
skewed_data <- rgamma(100, shape = 2, scale = 2)
hist(skewed_data)
# Perform a t-test against the null hypothesis that the mean is 0
t_test_result <- t.test(skewed_data, mu = 0)
# Perform a Wilcoxon test against the null hypothesis that the median is 0
# (Note: The Wilcoxon test requires non-negative data)
wilcox_test_result <- wilcox.test(skewed_data, mu = 0, conf.int = TRUE)
# Output the results of the t-test
print(t_test_result)
# Output the results of the Wilcoxon test
print(wilcox_test_result)
n <- 20
# Generate skewed data from a Gamma distribution
skewed_data <- rgamma(n, shape = 2, scale = 2)
hist(skewed_data)
t.test(skewed_data, mu = 0)
# Generate skewed data from a Gamma distribution
x <- rgamma(n, shape = 2, scale = 2)
# Generate skewed data from a Gamma distribution
x <- rgamma(n, shape = 2, scale = 2)
y <- rgamma(n, shape = 3, scale = 2)
mean(x)
mean(y)
n <- 20
# Generate skewed data from a Gamma distribution
x <- rgamma(n, shape = 2, scale = 2)
y <- rgamma(n, shape = 3, scale = 2)
mean(x)
mean(y)
wilcox.test(x, y, conf.int = TRUE)
# Generate skewed data from a Gamma distribution
x <- rgamma(n, shape = 2, scale = 2)
y <- rgamma(n, shape = 3, scale = 2)
mean(x)
t.test(x, y)
n <- 20000
# Generate skewed data from a Gamma distribution
x <- rgamma(n, shape = 2, scale = 2)
y <- rgamma(n, shape = 3, scale = 2)
mean(x)
mean(y)
hist(x)
hist(y)
n <- 20
# Generate skewed data from a Gamma distribution
x <- rgamma(n, shape = 2, scale = 2)
y <- rgamma(n, shape = 3, scale = 2)
mean(x)
mean(y)
t.test(x, y)
n <- 20
# Generate skewed data from a Gamma distribution
x <- rgamma(n, shape = 2, scale = 2)
y <- rgamma(n, shape = 3, scale = 2)
mean(x)
mean(y)
t.test(x, y)
wilcox.test(x, y, conf.int = TRUE)
?wilcox.test
set.seed(5)   ## damit wir alle dieselben Zufallszahlen haben
n <- 20   ## StichprobengrÃ¶sse
mu <- 3   ## mu
sigma <- 10   ## sigma
X <- rnorm(n, mu, sigma)   ## siehe ?rnorm
psych::describe(X)
mean(X) + c(-1, 1) * qt(0.975,n- 1) * sd(X)/sqrt(n)
t.test(X)
wilcox.test(X, conf.int = TRUE)
library(pacman)
p_load(tidyverse)
# Bsp 2)
data_frame <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsResource/master/Data/Ginzberg.csv')
median(data_frame$simplicity, na.rm = TRUE)
# Bsp 2)
data_frame <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsResource/master/Data/Ginzberg.csv')
median(data_frame$simplicity, na.rm = TRUE)
# Bsp 2)
df <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsResource/master/Data/Ginzberg.csv')
# Bsp 2)
df <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsResource/master/Data/Ginzberg.csv')
# Bsp 2)
df <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/Ginzberg.csv', sep=",")
median(data_frame$simplicity, na.rm = TRUE)
median(df$simplicity, na.rm = TRUE)
# a)
median(df$simplicity, na.rm = TRUE)
cor(df$simplicity, df$depression, use = "complete.obs")
# Read the data from the provided URL
df3 <- read_csv('https://raw.githubusercontent.com/mcdr65/StatsResource/master/Data/WeightLoss.csv', sep = ",")
# Read the data from the provided URL
df3 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsResource/master/Data/WeightLoss.csv', sep = ",")
# Read the data from the provided URL
df3 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsResource/master/Data/WeightLoss.csv', sep = ",")
# Read the data from the provided URL
df3 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/WeightLoss.csv', sep = ",")
# Read the data from the provided URL
df3 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/WeightLoss.csv', sep = ",")
# Split the data frame by 'group'
split_data <- split(df3, df3$group)
split_data
df3
split_data
# Calculate the mean weight loss after 1 month for the 'Diet' group
mean(split_data$Diet$wl1, na.rm = TRUE)
# Calculate the mean weight loss after 1 month for the 'Control' group
mean(split_data$Control$wl1, na.rm = TRUE)
# or
df3 %>% group_by(group) %>%
summarize(mean_wl1 = mean(wl1))
# _Bsp 4)----
pnorm(169.83, mean = 170.49, sd = sqrt(408.85))
# _Bsp 6) Regression----
df6 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/regression.
csv', sep = ",")
# _Bsp 6) Regression----
df6 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/regression.csv', sep = ",")
df6
summary(lm(y ~ x, data = df6))
plot(y ~ x, data = df6)
# Bsp 7) regressionDich----
df7 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/regressionDich.csv', sep = ",")
df7
str(df7)
# Bsp 7) regressionDich----
df7 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/regressionDich.csv', sep = ",", stringsAsFactors = TRUE)
str(df7)
reg2data$Treatment<-relevel(reg2data$Treatment,ref="Placeb")
df7$Treatment<-relevel(df7$Treatment,ref="Placeb")
?describe
describe_by(df7, Treatment)
p_load(tidyverse, psych)
describe_by(df7, Treatment)
?by
by(df7, Treatment)
by(df7, df7$Treatment)
by(df7$Response, df$Treatment, psych::describe)
by(df7$Response, df7$Treatment, psych::describe)
summary(Response ~ Treatment, data = df7)
summarylm((Response ~ Treatment, data = df7))
summary(lm(Response ~ Treatment, data = df7))
# between-group difference
t.test(Response ~ Treatment, data = df7)
df7
# between-group difference
mean(df7[Treatment == "Interv",]$Response, df7[Treatment == "Placeb",]$Response)
# between-group difference
df7 <- as.data.table(df7)
p_load(tidyverse, psych, data.table)
# between-group difference
df7 <- as.data.table(df7)
mean(df7[Treatment == "Interv",]$Response, df7[Treatment == "Placeb",]$Response)
df7[Treatment == "Interv",]$Response
df7[Treatment == "Placeb",]$Response
mean(df7[Treatment == "Interv",]$Response - df7[Treatment == "Placeb",]$Response)
# _Bsp 8) standardized height ----
# z =
scale(194.78)
# _Bsp 8) standardized height ----
# z =
(194.78 - 170.49)/30.22
# _Bsp 2) approx. KI----
qnorm(97.5)
# _Bsp 2) approx. KI----
qnorm(97.5)
# _Bsp 2) approx. KI----
qnorm(0.975)
# _Bsp 2) approx. KI----
z <- qnorm(0.975)
53.283 + c(-1,1)*z*sqrt(3.799)/sqrt(103)
# _Bsp 3) suche n
z <- qnorm(0.975)
sigma  <- 7
delta <- 2.9
# n =
z^2*sigma^2/delta^2
# _Bsp 3) suche n
z <- qnorm(0.975)
sigma  <- 7
delta <- 2.9
# n =
z^2*sigma^2/delta^2
# n =
z^2*sigma^2/delta^2
# n nominal
(n <- z^2*sigma^2/delta^2)
# n with dropout
n/0.7
n <- round(n)
n
# n with dropout
n/0.7
# _Bsp 4) exactes KI-----
59.94 + c(-1,1)*qt(1 - 0.1/2, df = 21)*sqrt(2.82)/sqrt(21)
# _Bsp 6) weight gender----
df6 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/regressionDich.csv', sep = ",", stringsAsFactors = TRUE)
# _Bsp 6) weight gender----
df6 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/omega.csv', sep = ",", stringsAsFactors = TRUE)
str(df6)
dataS <- split(data$weight,data$gender)
dataS <- split(df6$weight, df6$gender)
dataS
n1 <- length(dataS$female)
n2 <- length(dataS$male)
x1_mean <- mean(dataS[gender == "female",]$weight)
dataS <- as.data.table(dataS)
# _Bsp 6) weight gender----
df6 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/omega.csv', sep = ",", stringsAsFactors = TRUE)
str(df6)
dataS <- split(df6$weight, df6$gender)
dataS <- as.data.table(dataS)
dataS
# _Bsp 6) weight gender----
df6 <- read.csv('https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/omega.csv', sep = ",", stringsAsFactors = TRUE)
str(df6)
dataS <- split(df6$weight, df6$gender)
n1 <- length(dataS$female)
n2 <- length(dataS$male)
n1
n2
x1_mean <- subset(dataS, gender)
?subset
str(dataS)
x1_mean <- mean(dataS$female$weight)
x1_mean <- mean(dataS$female)
x1_ mean
x1_mean
x2_mean <- mean(dataS$male)
x2_mean
s1 <- sd(dataS$female)
s1
s2 <- sd(dataS$male)
s2
t.test(dataS$female, dataS$male)
t.test(dataS$male, dataS$female)
mean(dataS$male - dataS$female)
mean(dataS$male) - mean(dataS$female)
mean_diff <- mean(dataS$male) - mean(dataS$female)
# siehe Folie 32
mean_diff <- mean(dataS$male) - mean(dataS$female)
se <- sqrt(1/n1 + 1/n2)*sqrt(((n1-1)*s1^2 + (n2-1*s2^2))/(n1 + n2 - 2))
mean_diff + c(1,-1)*qt(0.975, n1 + n2 - 2)*se
# siehe Folie 32
mean_diff <- mean(dataS$male) - mean(dataS$female)
# siehe Folie 32
mean_diff <- mean(dataS$male) - mean(dataS$female)
se <- sqrt(1/n1 + 1/n2)*sqrt((n1-1)*s1^2 + (n2-1)*s2^2))/(n1 + n2 - 2))
se <- sqrt(1/n1 + 1/n2) * sqrt( ((n1-1)*s1^2 + (n2-1)*s2^2)/(n1 + n2 - 2) )
mean_diff + c(1,-1)*qt(0.975, n1 + n2 - 2)*se
# _Bsp 1) Expectation diskrete ZV----
# Define the values of X and their corresponding probabilities
values_of_X <- c(1, 2, 3, 4)
probabilities <- c(0.282, 0.221, 0.339, 0.158)
sum(values_of_X * probabilities)
# _Bsp 2)----
# (a)
23*0.6
# (b)
23*0.6*(1-0.6)
# (c)
sqrt(23*0.6*(1-0.6))
# _Bsp 3) Bivariat----
# Define the joint probability matrix
joint_probs <- matrix(c(0.125, 0.125, 0.000,
0.125, 0.250, 0.125,
0.000, 0.125, 0.125),
nrow = 3, byrow = TRUE)
joint_probs
# Values for X and Y
values_X <- 0:2
values_Y <- 0:2
# Calculate marginal distributions for X
marginal_X <- apply(joint_probs, 1, sum)
# Calculate marginal distributions for Y
marginal_Y <- apply(joint_probs, 2, sum)
# Calculate the expected values for X and Y
E_X <- sum(values_X * marginal_X)
E_Y <- sum(values_Y * marginal_Y)
# Calculate the expectation of XY
E_XY <- sum(joint_probs * outer(values_X, values_Y))
# Calculate the covariance between X and Y
cov_XY <- E_XY - (E_X * E_Y)
# Print the results
print(paste("E(X) =", E_X))
print(paste("E(Y) =", E_Y))
print(paste("Cov(X,Y) =", cov_XY))
outer(values_X, values_Y)
# _Bsp 4) Verschiebungssatz----
values_of_X <- c(1, 2, 3)
probabilities <- c(0.346, 0.178, 0.476)
E_X <- sum(values_of_X * probabilities)
E_X2 <- sum((values_of_X^2) * probabilities)
variance_X <- E_X2 - E_X^2
variance_X
# Set working directory to source file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load packages-----------------------------------------------------------------
library(pacman)
pacman::p_load(tidyverse, tictoc)
shape <- 2
scale <- 2
x <- rgamma(1000, shape, scale)
y <- rgamma(1000, shape, scale) # H_0 true, no location shift.
data <- data.frame(Value = c(x, y),
Group = factor(rep(c("X", "Y"), each = 1000)))
median_x <- median(x)
median_y <- median(y)
ggplot(data, aes(x = Value, fill = Group)) +
geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
geom_vline(xintercept = median_x, color = "blue", linetype = "dashed", size = 1) +
geom_vline(xintercept = median_y, color = "red", linetype = "dashed", size = 1) +
scale_fill_manual(values = c("blue", "red")) +
ggtitle("H_0 is true") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, size = 9),
legend.title = element_blank())  # Center the title
W_results <- c()
n_sim <- 10000
n <- 1000
tic()
for(i in 1:n_sim){
x <- rgamma(n, shape, scale)
y <- rgamma(n, shape, scale)
#x <- rnorm(n, mean = 10, sd = 3) # H_0 is true.
#y <- rnorm(n, mean = 10, sd = 3)
D <- (x - y)
R <- rank(abs(D))
Wplus <- sum(R[D > 0])
Wminus <- sum(R[D < 0])
W <- min(Wplus, Wminus) # this results in only negative Z!
#W <- ifelse(runif(1)>0.5, Wminus, Wplus)
#W <- Wplus # works too, see https://epub.ub.uni-muenchen.de/25569/1/BA_Steinherr.pdf
#W <- Wminus # works too
W_results <- append(W_results, W)
}
toc() # n_sim <- 100000 24.025 sec elapsed
EW <- 1/4*n*(n+1)
VarW <- n*(n+1)*(2*n+1)/24
Z <- (W_results - EW)/sqrt(VarW) # appr. N(0,1)
ggplot(data.frame(Z=Z), aes(x=Z)) +
geom_histogram(aes(y = ..density..), fill = "blue", color = "black") +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red", size = 1) +
ggtitle("Standardized W") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5), legend.title = element_blank()) +
annotate("text", x = Inf, y = Inf, label = paste("Mean(Z):",
round(mean(Z), 6), "\nSD(Z):", round(sd(Z), 6)),
hjust = 1.1, vjust = 1.1, size = 3, color = "black")
W_results <- c()
n_sim <- 10000
n <- 1000
tic()
for(i in 1:n_sim){
x <- rgamma(n, shape, scale)
y <- rgamma(n, shape, scale)
#x <- rnorm(n, mean = 10, sd = 3) # H_0 is true.
#y <- rnorm(n, mean = 10, sd = 3)
D <- (x - y)
R <- rank(abs(D))
Wplus <- sum(R[D > 0])
Wminus <- sum(R[D < 0])
#W <- min(Wplus, Wminus) # this results in only negative Z!
#W <- ifelse(runif(1)>0.5, Wminus, Wplus)
W <- Wplus # works too, see https://epub.ub.uni-muenchen.de/25569/1/BA_Steinherr.pdf
#W <- Wminus # works too
W_results <- append(W_results, W)
}
toc() # n_sim <- 100000 24.025 sec elapsed
EW <- 1/4*n*(n+1)
VarW <- n*(n+1)*(2*n+1)/24
Z <- (W_results - EW)/sqrt(VarW) # appr. N(0,1)
ggplot(data.frame(Z=Z), aes(x=Z)) +
geom_histogram(aes(y = ..density..), fill = "blue", color = "black") +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red", size = 1) +
ggtitle("Standardized W") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5), legend.title = element_blank()) +
annotate("text", x = Inf, y = Inf, label = paste("Mean(Z):",
round(mean(Z), 6), "\nSD(Z):", round(sd(Z), 6)),
hjust = 1.1, vjust = 1.1, size = 3, color = "black")
W_results <- c()
n_sim <- 10000
n <- 1000
tic()
for(i in 1:n_sim){
x <- rgamma(n, shape, scale)
y <- rgamma(n, shape, scale)
#x <- rnorm(n, mean = 10, sd = 3) # H_0 is true.
#y <- rnorm(n, mean = 10, sd = 3)
D <- (x - y)
R <- rank(abs(D))
Wplus <- sum(R[D > 0])
Wminus <- sum(R[D < 0])
#W <- min(Wplus, Wminus) # this results in only negative Z!
#W <- ifelse(runif(1)>0.5, Wminus, Wplus)
#W <- Wplus # works too, see https://epub.ub.uni-muenchen.de/25569/1/BA_Steinherr.pdf
W <- Wminus # works too
W_results <- append(W_results, W)
}
toc() # n_sim <- 100000 24.025 sec elapsed
EW <- 1/4*n*(n+1)
VarW <- n*(n+1)*(2*n+1)/24
Z <- (W_results - EW)/sqrt(VarW) # appr. N(0,1)
ggplot(data.frame(Z=Z), aes(x=Z)) +
geom_histogram(aes(y = ..density..), fill = "blue", color = "black") +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red", size = 1) +
ggtitle("Standardized W") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5), legend.title = element_blank()) +
annotate("text", x = Inf, y = Inf, label = paste("Mean(Z):",
round(mean(Z), 6), "\nSD(Z):", round(sd(Z), 6)),
hjust = 1.1, vjust = 1.1, size = 3, color = "black")
# Consider the t-Test
gr1 <- rnorm(40, mean = 0, sd = 1)
gr2 <- rnorm(40, mean = 1, sd = 1)
t.test(gr1, gr2)
# Introduce an outlier
gr2[3] <- gr2[3] + 100
mean(gr2)
# What happens to the p-Value and why?
t.test(gr1, gr2)$p.value
