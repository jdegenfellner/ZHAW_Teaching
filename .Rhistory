predict(d.ToyLogReg, newdata = data.frame(x=-4.910))
?predict
?predict.glm
predict.glm(d.ToyLogReg, newdata = data.frame(x=-4.910))
d.ToyLogReg
predict.glm(m.logreg, newdata = data.frame(x = -4.911), type = "response")
predict.glm(m.logreg, newdata = data.frame(x = -4.911), type = "response")
m.logreg <- glm(Y ~ x, family = "binomial", data = d.ToyLogReg)
summary(m.logreg)
# 27 (Residuals)
residuals(m.logreg)
dev_resid <- residuals(m.logreg, type = "deviance")
pearson_resid <- residuals(m.logreg, type = "pearson")
par(mfrow = c(2, 1))
plot(predict(m.logreg), dev_resid, ylab = "Deviance residuals")
plot(predict(m.logreg), pearson_resid, ylab = "Pearson residuals")
# with ggplot
d.ToyLogReg$resid_dev <- residuals(m.logreg, type = "deviance")
plot(m.logreg, which = 1)
par(mfrow = c(1, 1))
plot(m.logreg, which = 1)
# 29
d.hiv <- read.csv("https://raw.githubusercontent.com/mcdr65/StatsRsource/master/Data/HIV.csv")
str(d.hiv)
predict(m.logreg)
predict(m.logreg, type = "response")
logit <- function(p) {
log(p/(1-p))
}
predict(m.logreg, type = "response") # probabilities
logit(predict(m.logreg))
logistic <- function(x) {
1 / (1 + exp(-x))
}
logistic(predict(m.logreg))
logistic(predict(m.logreg)) - predict(m.logreg)
logistic(predict(m.logreg)) - predict(m.logreg, type = "response")
round(logistic(predict(m.logreg)) - predict(m.logreg, type = "response"))
library(brms)
library(rstanarm)
library(rstan)
library(mlbench)
library(rstanarm)
library(bayestestR)
library(bayesplot)
library(insight)
library(broom)
library(tidyverse)
#remotes::install_github("poisonalien/flatuicoloRs")
library(flatuicoloRs)
# Set working directory to source file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# READ----
d.lmm <- read.csv("./UEB_LMM_files/lmm1.csv", sep = ",", stringsAsFactors = TRUE) # adapt to your path
d.lmm$timeNum <- as.numeric(d.lmm$time)
str(d.lmm)
# previous model:
m.ri <- lmer(response ~ timeNum + (1|subject), data = d.lmm)
summary(m.ri)
# Estimate model-----
lmm_bayes <- stan_glmer(response ~ timeNum + (1 | subject), data = d.lmm, family = gaussian())
summary(lmm_bayes)
coef(lmm_bayes)
plot(lmm_bayes, pars = c("(Intercept)", "timeNum"))
mcmc_dens(lmm_bayes, pars = c("timeNum"))+
vline_at(1.5223, col="red")
# Plot density of POSTERIOR distributions (after applying MCMC)
mcmc_dens(lmm_bayes, pars = c("timeNum"))+
vline_at(1.5223, col="red")
mcmc_dens(lmm_bayes, pars = c("(Intercept)"))+
vline_at(20.2 , col="red")
point_estimate(lmm_bayes)
?point_estimate
plot(point_estimate(lmm_bayes)) # https://easystats.github.io/see/articles/bayestestR.html
hdi(lmm_bayes, ci = c(0.5, 0.75, 0.89, 0.95))
plot(hdi(lmm_bayes, ci = c(0.5, 0.75, 0.89, 0.95)))
point_estimate(lmm_bayes, centrality = "all") # centrality: The point-estimates (centrality indices) to compute. Character (vector) or list with one or more of these options: "median", "mean", "MAP" or "all".
plot(point_estimate(lmm_bayes)) # https://easystats.github.io/see/articles/bayestestR.html
point_estimate(lmm_bayes, centrality = "all") # centrality: The point-estimates (centrality indices) to compute. Character (vector) or list with one or more of these options: "median", "mean", "MAP" or "all".
point_estimate(lmm_bayes, centrality = "all") # centrality: The point-estimates (centrality indices) to compute. Character (vector) or list with one or more of these options: "median", "mean", "MAP" or "all".
point_estimate(lmm_bayes, centrality = "all") # centrality: The point-estimates (centrality indices) to compute. Character (vector) or list with one or more of these options: "median", "mean", "MAP" or "all".
# MAP = Maximum a posteriori estimation = Mode of the distribution
plot(point_estimate(lmm_bayes)) # https://easystats.github.io/see/articles/bayestestR.html
hdi(lmm_bayes, ci = c(0.5, 0.75, 0.89, 0.95))
plot(hdi(lmm_bayes, ci = c(0.5, 0.75, 0.89, 0.95)))
rope(lmm_bayes, ci = c(0.9, 0.95))
plot(rope(lmm_bayes, ci = c(0.9, 0.95)))
plot(rope(lmm_bayes, ci = c(0.9, 0.95)), rope_color = "red") +
scale_fill_brewer(palette = "Greens", direction = -1)
#Differences between the observed and simulated distributions suggest that the model might not be capturing some aspects of the data. However, it's important to note that this doesn't necessarily mean that the model is "wrong", just that there are aspects of the data that it doesn't capture perfectly.
pp_check(lmm_bayes) # does not look bad
stan_diag(lmm_bayes) # Diagnostics of the MCMC algorithm....
# Model comparison in Bayes-----------
lmm_bayes2 <- stan_glmer(response ~ timeNum + (timeNum | subject), data = d.lmm, family = gaussian())
loo1 <- loo(lmm_bayes)
loo2 <- loo(lmm_bayes2)
loo_compare(loo1, loo2) # For models fit using MCMC, compute approximate leave-one-out cross-validation (LOO, LOOIC) or, less preferably, the Widely Applicable Information Criterion (WAIC) using the loo package.
# Baysian hypothesis testing!-------
hypothesis(lmm_bayes, "timeNum = 0") # uses package brms
posterior_interval(lmm_bayes)
# Setzen Sie die Parameter der Beta-Prior-Verteilung
alpha_prior = 2
beta_prior = 2
# Setzen Sie die Anzahl der Würfe
n_values = c(3, 5, 10, 25, 100, 1000)
# Erstellen Sie einen Dataframe, um die Ergebnisse zu speichern
results = data.frame(n = integer(), frequentist = numeric(), bayesian = numeric())
# Führen Sie die Berechnungen für jede Anzahl von Würfen durch
for (n in n_values) {
# Berechnen Sie die frequentistische Schätzung (Anteil der "Kopf"-Ergebnisse)
frequentist_estimate = 1 / n
# Aktualisieren Sie die Beta-Verteilungsparameter für die Bayes'sche Schätzung
alpha_posterior = alpha_prior + 1
beta_posterior = beta_prior + n - 1
# Berechnen Sie die Bayes'sche Schätzung (Erwartungswert der Beta-Verteilung)
bayesian_estimate = alpha_posterior / (alpha_posterior + beta_posterior)
# Fügen Sie die Ergebnisse zum Dataframe hinzu
results = rbind(results, data.frame(n = n, frequentist = frequentist_estimate, bayesian = bayesian_estimate))
}
# Ausgabe der Ergebnisse
print(results)
# Setzen Sie die Parameter der Beta-Prior-Verteilung
alpha_prior = 2
beta_prior = 2
# Setzen Sie die Anzahl der Würfe
n_values = c(3, 5, 10, 25, 100)
# Erstellen Sie einen Dataframe, um die Ergebnisse zu speichern
results = data.frame(n = integer(), frequentist = numeric(), bayesian = numeric())
# Führen Sie die Berechnungen für jede Anzahl von Würfen durch
for (n in n_values) {
# Berechnen Sie die frequentistische Schätzung (Anteil der "Kopf"-Ergebnisse)
frequentist_estimate = 1 / n
# Aktualisieren Sie die Beta-Verteilungsparameter für die Bayes'sche Schätzung
alpha_posterior = alpha_prior + 1
beta_posterior = beta_prior + n - 1
# Berechnen Sie die Bayes'sche Schätzung (Erwartungswert der Beta-Verteilung)
bayesian_estimate = alpha_posterior / (alpha_posterior + beta_posterior)
# Fügen Sie die Ergebnisse zum Dataframe hinzu
results = rbind(results, data.frame(n = n, frequentist = frequentist_estimate, bayesian = bayesian_estimate))
# Aktualisieren Sie die Prior-Parameter für den nächsten Durchgang
alpha_prior = alpha_posterior
beta_prior = beta_posterior
}
# Ausgabe der Ergebnisse
print(results)
beta <- 0.1
(0.1*beta+0.9*0.05)/(0.1*(1-beta)+0.1*beta+0.9*0.05+0.9*0.95)
data("attitude")
str(attitude)
suppressPackageStartupMessages(library(mlbench))
suppressPackageStartupMessages(library(rstanarm))
suppressPackageStartupMessages(library(bayestestR))
suppressPackageStartupMessages(library(bayesplot))
suppressPackageStartupMessages(library(insight))
suppressPackageStartupMessages(library(broom))
data("BostonHousing")
str(BostonHousing)
suppressPackageStartupMessages(library(mlbench))
suppressPackageStartupMessages(library(rstanarm))
suppressPackageStartupMessages(library(bayestestR))
suppressPackageStartupMessages(library(bayesplot))
suppressPackageStartupMessages(library(insight))
suppressPackageStartupMessages(library(broom))
library(tidyverse)
data("BostonHousing")
str(BostonHousing)
bost <- BostonHousing[,c("medv","age","dis","chas")]
summary(bost)
?BostonHousing # for details about the data!
# 1) Classical linear regression model----
model_freq <- lm(medv ~., data = bost)
summary(model_freq)
install.packages("mlbench")
?stan_glm
# 2) Bayesian regression----
model_bayes<- stan_glm(medv ~., data = bost, seed = 111)
suppressPackageStartupMessages(library(mlbench))
suppressPackageStartupMessages(library(rstanarm))
suppressPackageStartupMessages(library(bayestestR))
suppressPackageStartupMessages(library(bayesplot))
suppressPackageStartupMessages(library(insight))
suppressPackageStartupMessages(library(broom))
data("BostonHousing")
str(BostonHousing)
bost <- BostonHousing[,c("medv","age","dis","chas")]
summary(bost)
?BostonHousing # for details about the data!
# 1) Classical linear regression model----
model_freq <- lm(medv ~., data = bost)
summary(model_freq)
confint(model_freq) # recht aehnlich wie Bayes
# 2) Bayesian regression----
model_bayes<- stan_glm(medv ~., data = bost, seed = 111)
# 2) Bayesian regression----
model_bayes<- stan_glm(medv ~., data = bost, seed = 111)
print(model_bayes, digits = 3)
# Predict individual observation
df <- data.frame(medv = 76.5, age = 34, dis = 5.03, chas = 1)
df$chas <- as.factor(df$chas)
predict(model_freq, newdata = df) # 34.15939
class(model_freq)
class(model_bayes)
predict(model_freq, newdata = df) # 34.15939
mcmc_dens(model_bayes, pars = c("age"))+ # von der posterior
vline_at(-0.143, col="red")
mcmc_dens(model_bayes, pars=c("chas1"))+
vline_at(7.496, col="red")
mcmc_dens(model_bayes, pars=c("dis"))+
vline_at(-0.244, col="red") + ggtitle("posterior distribution of chas1")
library(tidyverse)
mcmc_dens(model_bayes, pars=c("dis"))+
vline_at(-0.244, col="red") + ggtitle("posterior distribution of chas1")
mcmc_dens(model_bayes, pars=c("dis"))+
vline_at(-0.244, col="red") +
ggtitle("posterior distribution of chas1")
mcmc_dens(model_bayes, pars=c("chas1"))+
vline_at(7.496, col="red")
mcmc_dens(model_bayes, pars=c("dis"))+
vline_at(-0.244, col="red") +
ggtitle("posterior distribution of chas1")
flextable(describe_posterior(model_bayes,
ci_method = "HDI", # Highest Density Interval (HDI), All points within this interval have a higher probability density than points outside the interval. The HDI can be used in the context of uncertainty characterisation of posterior distributions as Credible Interval (CI).
rope_range = "default", # = sd(bost$medv)*0.1 ... If "default", the bounds are set to x +- 0.1*SD(response)
diagnostic = NULL))
library(flextable)
flextable(describe_posterior(model_bayes,
ci_method = "HDI", # Highest Density Interval (HDI), All points within this interval have a higher probability density than points outside the interval. The HDI can be used in the context of uncertainty characterisation of posterior distributions as Credible Interval (CI).
rope_range = "default", # = sd(bost$medv)*0.1 ... If "default", the bounds are set to x +- 0.1*SD(response)
diagnostic = NULL))
sd(bost$medv)*0.1
hdi(model_bayes)
?hdi
hdi(model_bayes, ci = 0.99)
plot(p_direction(model_bayes))
pd_to_p(0.81925)
mcmc_dens(model_bayes, pars=c("chas1"))+
vline_at(7.496, col="red")
print(model_bayes, digits = 3)
summary(model_freq)
pd_to_p(0.81925)
# Posterior Predictive Check:
ppc_data <- posterior_predict(model_bayes) # Generate posterior predictive data
ppc_dens_overlay(y = bost$medv, yrep = ppc_data) # Plot
# Further vizualisations
# https://easystats.github.io/see/articles/bayestestR.html
result <- estimate_density(model_bayes, select=c("dis","chas"))
plot(result)
plot(result, stack = FALSE, priors = TRUE)
result <- describe_posterior(model_bayes)
plot(result)
y1 <- c(-.5,0,1.2,1.2,1.2,1.9,2.4,3)*100
y2 <- c(-1.2,-1.2,-.5,0,0,.5,1.1,1.9)*100
y1 <- c(-.5,0,1.2,1.2,1.2,1.9,2.4,3)*100
y2 <- c(-1.2,-1.2,-.5,0,0,.5,1.1,1.9)*100
data <- data.frame(y1,y2)
psych::describe(data)
a <- 20 #ROPE
y1 <- c(-.5,0,1.2,1.2,1.2,1.9,2.4,3)*100
y2 <- c(-1.2,-1.2,-.5,0,0,.5,1.1,1.9)*100
data <- data.frame(y1,y2)
psych::describe(data)
t.test(y1, y2, mu = 0, var.equal = FALSE)
t.test(y1, y2, mu = 0) # takes the correct t-Test automatically
library(BEST)
?hdi.bugs
?hdi
BESTout <- BESTmcmc(y1, y2, verbose=FALSE)
par(bg = "yellow")
pairs(BESTout)
summary(BESTout,ROPEm = c(-a+c, c+a), compValm = c, digits = 5)
par(bg = "yellow")
plot(BESTout, compVal = c, ROPE = c(-a+c, c+a), showCurve = FALSE)
library(BEST)
a <- 20 #ROPE
c <- 0 #delta_0 # der praktisch relevante Unterschied muss groesser als 20 sein!
BESTout <- BESTmcmc(y1, y2, verbose=FALSE)
par(bg = "yellow")
pairs(BESTout)
summary(BESTout,ROPEm = c(-a+c, c+a), compValm = c, digits = 5)
par(bg = "yellow")
plot(BESTout, compVal = c, ROPE = c(-a+c, c+a), showCurve = FALSE)
par(bg = "yellow")
plot(BESTout, which = "effect", showCurve = FALSE)
par(bg = "yellow")
plot(BESTout, compVal = c, ROPE = c(-a+c, c+a), showCurve = FALSE)
library(brms)
library(rstanarm)
library(rstan)
library(mlbench)
library(rstanarm)
library(bayestestR)
library(bayesplot)
library(insight)
library(broom)
library(tidyverse)
#remotes::install_github("poisonalien/flatuicoloRs")
library(flatuicoloRs)
# Set working directory to source file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# READ----
d.lmm <- read.csv("./UEB_LMM_files/lmm1.csv", sep = ",", stringsAsFactors = TRUE) # adapt to your path
d.lmm$timeNum <- as.numeric(d.lmm$time)
str(d.lmm)
# previous model:
m.ri <- lmer(response ~ timeNum + (1|subject), data = d.lmm)
summary(m.ri)
# Estimate model-----
lmm_bayes <- stan_glmer(response ~ timeNum + (1 | subject), data = d.lmm, family = gaussian())
summary(lmm_bayes)
coef(lmm_bayes)
plot(lmm_bayes, pars = c("(Intercept)", "timeNum"))
# Plot density of POSTERIOR distributions (after applying MCMC)
mcmc_dens(lmm_bayes, pars = c("timeNum")) +
vline_at(1.5223, col="red")
mcmc_dens(lmm_bayes, pars = c("(Intercept)")) +
vline_at(20.2 , col="red")
point_estimate(lmm_bayes, centrality = "all") # centrality: The point-estimates (centrality indices) to compute. Character (vector) or list with one or more of these options: "median", "mean", "MAP" or "all".
# MAP = Maximum a posteriori estimation = Mode of the distribution
plot(point_estimate(lmm_bayes)) # https://easystats.github.io/see/articles/bayestestR.html
hdi(lmm_bayes, ci = c(0.5, 0.75, 0.89, 0.95))
plot(hdi(lmm_bayes, ci = c(0.5, 0.75, 0.89, 0.95)))
rope(lmm_bayes, ci = c(0.9, 0.95))
plot(rope(lmm_bayes, ci = c(0.9, 0.95)))
plot(rope(lmm_bayes, ci = c(0.9, 0.95)), rope_color = "red") +
scale_fill_brewer(palette = "Greens", direction = -1)
#Differences between the observed and simulated distributions suggest that the model might not be capturing some aspects of the data. However, it's important to note that this doesn't necessarily mean that the model is "wrong", just that there are aspects of the data that it doesn't capture perfectly.
pp_check(lmm_bayes) # does not look bad
stan_diag(lmm_bayes) # Diagnostics of the MCMC algorithm....
# Model comparison in Bayes-----------
lmm_bayes2 <- stan_glmer(response ~ timeNum + (timeNum | subject), data = d.lmm, family = gaussian())
loo1 <- loo(lmm_bayes)
loo2 <- loo(lmm_bayes2)
loo_compare(loo1, loo2) # For models fit using MCMC, compute approximate
# Baysian hypothesis testing!-------
hypothesis(lmm_bayes, "timeNum = 0") # uses package brms
loo_compare(loo1, loo2) # For models fit using MCMC, compute approximate
compare_versions <- function() {
# Compares your current R Version to the CRAN-version.
library(rvest)
url <- "https://cran.r-project.org/bin/windows/base/"
page <- read_html(url)
text <- html_text(page)
cran_version <- substr(text, 12,16)
my_version <- substr(R.version.string, 11,15)
if (cran_version == my_version) {
message("Your R version is up to date.")
} else {
message(sprintf("Your R version is outdated. The latest version is %s.", cran_version))
}
}
compare_versions()
library(brms)
library(rstanarm)
library(rstan)
library(mlbench)
library(rstanarm)
library(bayestestR)
library(bayesplot)
library(insight)
library(broom)
library(tidyverse)
#remotes::install_github("poisonalien/flatuicoloRs")
library(flatuicoloRs)
# Set working directory to source file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# READ----
d.lmm <- read.csv("./UEB_LMM_files/lmm1.csv", sep = ",", stringsAsFactors = TRUE) # adapt to your path
d.lmm$timeNum <- as.numeric(d.lmm$time)
str(d.lmm)
# previous model:
m.ri <- lmer(response ~ timeNum + (1|subject), data = d.lmm)
summary(m.ri)
# Estimate model-----
lmm_bayes <- stan_glmer(response ~ timeNum + (1 | subject), data = d.lmm, family = gaussian())
coef(lmm_bayes)
plot(lmm_bayes, pars = c("(Intercept)", "timeNum"))
# Plot density of POSTERIOR distributions (after applying MCMC)
mcmc_dens(lmm_bayes, pars = c("timeNum")) +
vline_at(1.5223, col="red")
mcmc_dens(lmm_bayes, pars = c("(Intercept)")) +
vline_at(20.2 , col="red")
# Model comparison in Bayes-----------
lmm_bayes2 <- stan_glmer(response ~ timeNum + (timeNum | subject), data = d.lmm, family = gaussian())
loo1 <- loo(lmm_bayes)
loo2 <- loo(lmm_bayes2)
loo_compare(loo1, loo2) # For models fit using MCMC, compute approximate
# Baysian hypothesis testing!-------
hypothesis(lmm_bayes, "timeNum = 0") # uses package brms
#Differences between the observed and simulated distributions suggest that the model might not be capturing some aspects of the data. However, it's important to note that this doesn't necessarily mean that the model is "wrong", just that there are aspects of the data that it doesn't capture perfectly.
pp_check(lmm_bayes) # does not look bad
suppressPackageStartupMessages(library(rstanarm))
suppressPackageStartupMessages(library(bayestestR))
suppressPackageStartupMessages(library(bayesplot))
suppressPackageStartupMessages(library(insight))
suppressPackageStartupMessages(library(broom))
data("BostonHousing")
str(BostonHousing)
bost <- BostonHousing[,c("medv","age","dis","chas")]
summary(bost)
?BostonHousing # for details about the data!
# 1) Classical linear regression model----
model_freq <- lm(medv ~., data = bost)
summary(model_freq)
confint(model_freq) # recht aehnlich wie Bayes
# 2) Bayesian regression----
model_bayes<- stan_glm(medv ~., data = bost, seed = 111)
print(model_bayes, digits = 3)
prior_summary(model_bayes)
# Predict individual observation
df <- data.frame(medv = 76.5, age = 34, dis = 5.03, chas = 1)
df$chas <- as.factor(df$chas)
predict(model_bayes, newdata = df) # 34.17305
predict(model_freq, newdata = df) # 34.15939
mcmc_dens(model_bayes, pars = c("age"))+ # von der posterior
vline_at(-0.143, col="red")
mcmc_dens(model_bayes, pars=c("chas1"))+
vline_at(7.496, col="red")
mcmc_dens(model_bayes, pars=c("dis"))+
vline_at(-0.244, col="red") +
ggtitle("posterior distribution of chas1")
library(flextable)
flextable(describe_posterior(model_bayes,
ci_method = "HDI", # Highest Density Interval (HDI), All points within this interval have a higher probability density than points outside the interval. The HDI can be used in the context of uncertainty characterisation of posterior distributions as Credible Interval (CI).
rope_range = "default", # = sd(bost$medv)*0.1 ... If "default", the bounds are set to x +- 0.1*SD(response)
diagnostic = NULL))
hdi(model_bayes, ci = 0.99)
plot(p_direction(model_bayes))
pd_to_p(0.81925) # nicht ganz exakt ...
# Posterior Predictive Check:
ppc_data <- posterior_predict(model_bayes) # Generate posterior predictive data
ppc_dens_overlay(y = bost$medv, yrep = ppc_data) # Plot
# Further vizualisations
# https://easystats.github.io/see/articles/bayestestR.html
result <- estimate_density(model_bayes, select=c("dis","chas"))
plot(result)
plot(result, stack = FALSE, priors = TRUE)
result <- describe_posterior(model_bayes)
plot(result)
result <- p_direction(model_bayes)
plot(result)
result <- p_direction(model_bayes)
plot(result, priors = TRUE)
result <- p_significance(model_bayes)
result
plot(result)
?p_significance
?stan_glm
# Versuche mit anderen priors zu arbeiten (z.B. mit
# a) uninformative (prior = NULL) oder b)
model_bayes_uninf_prior <- stan_glm(medv ~., data = bost, prior = NULL)
summary(model_bayes_uninf_prior)
summary(model_bayes)
coef(model_bayes)
coef(model_bayes_uninf_prior)
# a) uninformative (prior = NULL) oder b)
model_bayes_uninf_prior <- stan_glm(medv ~., data = bost,
prior = NULL,
prior_intercept = NULL)
#summary(model_bayes_uninf_prior)
coef(model_bayes)
#summary(model_bayes)
coef(model_bayes_uninf_prior)
?stan_glm
# b) schiefen Verteilungen (z.B. Beta-Verteilung mit alpha = 2, beta = 5)
model_bayes_skewed_prior <- stan_glm(medv ~., data = bost,
prior = beta(2,5),
prior_intercept = NULL)
bost
str(bost)
list(3)
# b) schiefen Verteilungen (z.B. Beta-Verteilung mit alpha = 2, beta = 5)
model_bayes_skewed_prior <- stan_glm(medv ~., data = bost,
prior = list(beta(2,5),beta(2,5),beta(2,5)),
prior_intercept = NULL)
model_bayes_skewed_prior <- stan_glm(
medv ~ .,
data = bost,
prior = student_t(3, 0, 10),
prior_intercept = NULL
)
coef(model_bayes_skewed_prior)
coef(model_bayes)
?student_t
plot(student_t(3, 0, 10))
student_t(3, 0, 10)
density(student_t(3, 0, 10))
library(brms)
library(rstanarm)
library(rstan)
library(mlbench)
library(rstanarm)
library(bayestestR)
library(bayesplot)
library(insight)
library(broom)
library(tidyverse)
#remotes::install_github("poisonalien/flatuicoloRs")
library(flatuicoloRs)
# Set working directory to source file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# READ----
d.lmm <- read.csv("./UEB_LMM_files/lmm1.csv", sep = ",", stringsAsFactors = TRUE) # adapt to your path
d.lmm$timeNum <- as.numeric(d.lmm$time)
str(d.lmm)
# previous model:
m.ri <- lmer(response ~ timeNum + (1|subject), data = d.lmm)
summary(m.ri)
# Estimate model-----
lmm_bayes <- stan_glmer(response ~ timeNum + (1 | subject), data = d.lmm, family = gaussian())
summary(lmm_bayes)
coef(lmm_bayes)
install.packages("usethis")
